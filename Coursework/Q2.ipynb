{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856044c4",
   "metadata": {},
   "source": [
    "### The model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "acd4e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "from lightning.pytorch import LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import neurogym as ngym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30fd38d",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\mathbf{a}(t+\\Delta t) = \\mathbf{a}(t) + \\Delta \\mathbf{a} &= \\mathbf{a}(t) + \\frac{\\Delta t}{\\tau}[-\\mathbf{a}(t) + f(W_{a\\rightarrow a} \\mathbf{a}(t) + W_{x\\rightarrow a}  \\mathbf{x}(t) + \\mathbf{b}_r)] \\\\\n",
    "    &= (1 - \\frac{\\Delta t}{\\tau})\\mathbf{a}(t) + \\frac{\\Delta t}{\\tau}f(W_{a\\rightarrow a} \\mathbf{a}(t) + W_{x\\rightarrow a}  \\mathbf{x}(t) + \\mathbf{b}_r)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c787624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyRNN(nn.Module):\n",
    "    def __init__(self, num_input, num_hidden, delta_t = 0.05, tau = 100):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # dont need num_out for the layer probably\n",
    "        self.input_size = num_input\n",
    "        self.hidden_size = num_hidden\n",
    "\n",
    "        self.delta_t = delta_t\n",
    "        self.tau = tau\n",
    "        self.alpha = self.delta_t / self.tau\n",
    "\n",
    "        # linear or parameter?\n",
    "        self.inh = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hh = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.nonlinearity = nn.LeakyReLU()\n",
    "\n",
    "\n",
    "    def _init_hidden(self, input_shape):\n",
    "        batch_size = input_shape[1]\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "    def recurrent(self, input, hidden):\n",
    "        # input = x(t)\n",
    "        # hidden = a(t)\n",
    "        # hidden_new = a(t + delta_t)\n",
    "\n",
    "        # W_{a,a} & W_{x,a} are singular with no bias\n",
    "        hidden_new = self.nonlinearity(self.inh(input) + self.hh(hidden))\n",
    "\n",
    "        # hidden is a(t), activity of current neuron at time t\n",
    "        hidden_new = (1 - self.alpha) * hidden + self.alpha * hidden_new\n",
    "\n",
    "        return hidden_new\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "\n",
    "        # dont know what this does really\n",
    "        if hidden is None:\n",
    "            hidden = self._init_hidden(input.shape).to(input.device)\n",
    "\n",
    "        output = []\n",
    "        for i in range(len(input)):\n",
    "            hidden = self.recurrent(input[i], hidden)\n",
    "            output.append(hidden)\n",
    "        \n",
    "        output = torch.stack(output, dim=0)     # seq_len, batch, hidden_size\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "class RNNNet(LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # leaky layer\n",
    "        self.rnn = LeakyRNN(input_size, hidden_size)\n",
    "\n",
    "        # linear output layer\n",
    "        self.ho = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # loss func\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.lr = 0.01\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        leaky_out, _ = self.rnn(input)\n",
    "        output = self.ho(leaky_out)\n",
    "\n",
    "        return output, leaky_out\n",
    "\n",
    "\n",
    "    def training_step(self, batch, x_idx):\n",
    "        x, y = batch\n",
    "        output, _ = self.forward(x)\n",
    "        # print(f'input shape {x.shape}')\n",
    "        # print(f'output shape {y.shape}')\n",
    "        output = output.view(-1, self.output_size)\n",
    "        # output_last = output[-1]\n",
    "        loss = self.loss(output, y)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, x_idx):\n",
    "        x, y = batch\n",
    "        output, _ = self.forward(x)\n",
    "        output = output.view(-1, self.output_size)\n",
    "        test_loss = self.loss(output, y)\n",
    "\n",
    "        self.log('test_loss', test_loss, batch_size=x.shape[1])\n",
    "\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bbcb713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (SeqLen, Batch, Dim) = (100, 64, 3)\n",
      "Target has shape (SeqLen, Batch) = (100, 64)\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "task = 'PerceptualDecisionMaking-v0'\n",
    "kwargs = {'dt': 100}\n",
    "seq_len = 100\n",
    "batch_size = 64\n",
    "\n",
    "# dont mind the warnings\n",
    "ngym_dataset = ngym.Dataset(task, kwargs, batch_size, seq_len)\n",
    "env = ngym_dataset.env\n",
    "inputs, target = ngym_dataset()\n",
    "\n",
    "print('Input has shape (SeqLen, Batch, Dim) =', inputs.shape)\n",
    "print('Target has shape (SeqLen, Batch) =', target.shape)\n",
    "\n",
    "inputs = torch.from_numpy(inputs).type(torch.float)\n",
    "target = torch.from_numpy(target.flatten()).type(torch.long)\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "print(input_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd8ee3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgymWrapper(IterableDataset):\n",
    "    \"\"\"\n",
    "    Wrapper for converting neurogym dataset into in a pytorch dataset with tensors not numpy arrays\n",
    "    Kinda weird that ngym doesn't return tensors\n",
    "    todo: allow multiple batches per epoch\n",
    "        allow multiple datasets\n",
    "        should sample randomly?\n",
    "        can i handle batch size from here? maybe if i have __init__ build from ngym instead of passing the prebuilt dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        inputs, targets = self.dataset()\n",
    "        inputs = torch.from_numpy(inputs).type(torch.float)\n",
    "        targets = torch.from_numpy(targets.flatten()).type(torch.long)\n",
    "        \n",
    "        # never seen yield before, returns the object(s) without exiting the function\n",
    "        yield (inputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "550e9946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | rnn  | LeakyRNN         | 4.4 K  | train\n",
      "1 | ho   | Linear           | 195    | train\n",
      "2 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNNet(\n",
      "  (rnn): LeakyRNN(\n",
      "    (inh): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (hh): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (ho): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n",
      "Epoch 24: |          | 1/? [00:00<00:00, 11.67it/s, v_num=4, train_loss=0.253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: |          | 1/? [00:00<00:00, 10.33it/s, v_num=4, train_loss=0.253]\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "trainer_kwargs = {'max_epochs': 25,\n",
    "                  'logger': TensorBoardLogger('logs/')\n",
    "                  }\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "\n",
    "dataset = NgymWrapper(ngym_dataset)\n",
    "hidden_size = 64\n",
    "\n",
    "\n",
    "model = RNNNet(input_size, hidden_size, output_size)\n",
    "print(model)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd378f",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Testing the model with a new, unseen batch of ngym data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed4ca3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: |          | 1/? [00:00<00:00, 25.25it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_loss           0.20977191627025604\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.20977191627025604}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ngym_dataset = ngym.Dataset(task, kwargs, batch_size, seq_len)\n",
    "\n",
    "test_dataset = NgymWrapper(test_ngym_dataset)\n",
    "\n",
    "trainer.test(model, dataloaders=test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea34a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cog-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
